{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Finance related operations\n",
    "import pandas_datareader\n",
    "\n",
    "# Import this to silence a warning when converting data column of a dataframe on the fly\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are loading the every csv file separately from the Data folder in the project directory. Once we have stored them in our Notebook we merge them together to form a larger dataframe containing all the information we have available."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv('Data/2014_Financial_Data.csv', index_col=0)\n",
    "df_2015 = pd.read_csv('Data/2015_Financial_Data.csv', index_col=0)\n",
    "df_2016 = pd.read_csv('Data/2016_Financial_Data.csv', index_col=0)\n",
    "df_2017 = pd.read_csv('Data/2017_Financial_Data.csv', index_col=0)\n",
    "df_2018 = pd.read_csv('Data/2018_Financial_Data.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add new YEAR column to each of the dataframes to later being able to group the rows by their year. We add them to an array, so we can iterate over them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "df_2014['year'] = '2014'\n",
    "df_2015['year'] = '2015'\n",
    "df_2016['year'] = '2016'\n",
    "df_2017['year'] = '2017'\n",
    "df_2018['year'] = '2018'\n",
    "\n",
    "dataframes = [df_2014, df_2015, df_2016, df_2017, df_2018]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Merging of datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3808 entries, PG to WTT\n",
      "Columns: 225 entries, Revenue to year\n",
      "dtypes: float64(222), int64(1), object(2)\n",
      "memory usage: 6.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4120 entries, PG to WTT\n",
      "Columns: 225 entries, Revenue to year\n",
      "dtypes: float64(222), int64(1), object(2)\n",
      "memory usage: 7.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4797 entries, PG to WTT\n",
      "Columns: 225 entries, Revenue to year\n",
      "dtypes: float64(222), int64(1), object(2)\n",
      "memory usage: 8.3+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4960 entries, PG to WTT\n",
      "Columns: 225 entries, Revenue to year\n",
      "dtypes: float64(222), int64(1), object(2)\n",
      "memory usage: 8.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4392 entries, CMCSA to ZYME\n",
      "Columns: 225 entries, Revenue to year\n",
      "dtypes: float64(222), int64(1), object(2)\n",
      "memory usage: 7.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for df in dataframes:\n",
    "    print(df.info())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the datasets are quite similar. The only thing that varies between them is the number of rows. As we want to append all the datasets to form a bigger dataset we expect 3808 + 4120 + 4797 + 4960 + 4392 = 22'077 rows in the generated dataset.\n",
    "\n",
    "Looking at the column data types we have:\n",
    "- 222 numeric (financial indicators)\n",
    "- 1 integer (the **class** column)\n",
    "- 2 objects (**Sector and Price Change** column)\n",
    "\n",
    "In order to be able to append all the dataframes together we have to make sure all the columns have the same naming. It looks like the only column that is named differently from all the others across the datasets is the price change column. We are renaming it to 'PRICE CHANGE [%]'.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "year = 2015\n",
    "for df in dataframes:\n",
    "    df.rename(columns = {f'{year} PRICE VAR [%]': 'PRICE CHANGE [%]'}, inplace=True)\n",
    "    year += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check our assumption and see if now all the columns really are the same we use the numpy array_equal function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df 2014 and 2015 are equal: True\n",
      "Columns in df 2015 and 2016 are equal: True\n",
      "Columns in df 2016 and 2017 are equal: True\n",
      "Columns in df 2017 and 2018 are equal: True\n"
     ]
    }
   ],
   "source": [
    "columns_2014 = df_2014.columns.values.tolist()\n",
    "columns_2015 = df_2015.columns.values.tolist()\n",
    "columns_2016 = df_2016.columns.values.tolist()\n",
    "columns_2017 = df_2017.columns.values.tolist()\n",
    "columns_2018 = df_2018.columns.values.tolist()\n",
    "\n",
    "print(f'Columns in df 2014 and 2015 are equal: {np.array_equal(columns_2014, columns_2015)}')\n",
    "print(f'Columns in df 2015 and 2016 are equal: {np.array_equal(columns_2015, columns_2016)}')\n",
    "print(f'Columns in df 2016 and 2017 are equal: {np.array_equal(columns_2016, columns_2017)}')\n",
    "print(f'Columns in df 2017 and 2018 are equal: {np.array_equal(columns_2017, columns_2018)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see that all computations output TRUE. By reflexivity we know that the columns of all the datasets now have the same values. Now we can start appending them to form the mega-dataset we want to start working with."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "df = pd.concat(dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected we get a new dataframe with 22'077 entries and the common number of columns (225) of the individual datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Start cleaning\n",
    "Now that we have some information at one place we can start cleaning the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Price Change outliers\n",
    "As Price change is probably our most important feature we want to prepare that one first. This section is strongly inspired by an analysis done by Nicolas Carbone and can be found [HERE](https://www.kaggle.com/cnic92/explore-and-clean-financial-indicators-dataset)\n",
    "We are looking for major peaks/valleys, which indicate stocks that increased/decreased in value by an incredible amount with respect to the overall sector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Consumer Defensive' 'Basic Materials' 'Healthcare' 'Consumer Cyclical'\n",
      " 'Industrials' 'Real Estate' 'Communication Services' 'Energy'\n",
      " 'Financial Services' 'Utilities' 'Technology']\n",
      "['Consumer Defensive' 'Basic Materials' 'Healthcare' 'Consumer Cyclical'\n",
      " 'Industrials' 'Real Estate' 'Communication Services' 'Energy'\n",
      " 'Financial Services' 'Utilities' 'Technology']\n",
      "['Consumer Defensive' 'Basic Materials' 'Healthcare' 'Consumer Cyclical'\n",
      " 'Industrials' 'Real Estate' 'Communication Services' 'Energy'\n",
      " 'Financial Services' 'Utilities' 'Technology']\n",
      "['Consumer Defensive' 'Basic Materials' 'Healthcare' 'Consumer Cyclical'\n",
      " 'Industrials' 'Real Estate' 'Communication Services' 'Energy'\n",
      " 'Financial Services' 'Utilities' 'Technology']\n",
      "['Consumer Cyclical' 'Energy' 'Technology' 'Industrials'\n",
      " 'Financial Services' 'Basic Materials' 'Communication Services'\n",
      " 'Consumer Defensive' 'Healthcare' 'Real Estate' 'Utilities']\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "# Get the columns we need for this analysis for every year\n",
    "price_2014 = df_2014.loc[:, ['Sector', 'PRICE CHANGE [%]']]\n",
    "\n",
    "sectors = []\n",
    "\n",
    "for df in dataframes:\n",
    "    print(df['Sector'].unique())\n",
    "    sectors.append(df['Sector'].unique())\n",
    "\n",
    "for sector in sectors:\n",
    "    print(np.array_equal(sector, sectors[0]))\n",
    "\n",
    "for i in sectors[4]:\n",
    "    print('here')\n",
    "    if i not in sectors[0]:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}